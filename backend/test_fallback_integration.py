"""
æœ¬åœ°è§„åˆ™åº“é™çº§åŠŸèƒ½ - é›†æˆæµ‹è¯•ï¼ˆç®€åŒ–ç‰ˆï¼‰

ä¸ä¾èµ– pytestï¼Œå¯ä»¥ç›´æ¥è¿è¡Œ
"""
import asyncio
import sys
import os
import json
import time
from pathlib import Path
from unittest.mock import patch

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.services.health_monitor_service import HealthMonitorService, init_health_monitor
from app.services.local_rules_engine import LocalRulesEngine, init_local_rules_engine
from app.core.rules_config import RulesConfigManager


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.total_tests = 0
        self.passed_tests = 0
        self.failed_tests = 0
    
    def run_test(self, test_name, test_func):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        self.total_tests += 1
        
        try:
            asyncio.run(test_func())
            self.passed_tests += 1
            return True
        except AssertionError as e:
            print(f"\n  âœ— æµ‹è¯•å¤±è´¥: {test_name}")
            print(f"    æ–­è¨€é”™è¯¯: {e}")
            self.failed_tests += 1
            return False
        except Exception as e:
            print(f"\n  âœ— æµ‹è¯•å¼‚å¸¸: {test_name}")
            print(f"    é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            self.failed_tests += 1
            return False
    
    def print_summary(self):
        """æ‰“å°æµ‹è¯•æ€»ç»“"""
        print("\n" + "=" * 70)
        print("æµ‹è¯•æ€»ç»“")
        print("=" * 70)
        print(f"æ€»æµ‹è¯•æ•°: {self.total_tests}")
        print(f"é€šè¿‡: {self.passed_tests}")
        print(f"å¤±è´¥: {self.failed_tests}")
        print(f"æˆåŠŸç‡: {(self.passed_tests / self.total_tests * 100):.1f}%")
        print("=" * 70)
        
        if self.failed_tests == 0:
            print("\nğŸ‰ æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡ï¼é™çº§åŠŸèƒ½å·¥ä½œæ­£å¸¸ã€‚")
            return 0
        else:
            print(f"\nâš ï¸  æœ‰ {self.failed_tests} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
            return 1


# ============================================================================
# æµ‹è¯•1: AI æœåŠ¡å¤±è´¥è§¦å‘é™çº§
# ============================================================================

async def test_ai_failure_triggers_fallback():
    """æµ‹è¯• AI æœåŠ¡å¤±è´¥è§¦å‘é™çº§"""
    print("\n=== æµ‹è¯•1: AI æœåŠ¡å¤±è´¥è§¦å‘é™çº§ ===")
    
    # åˆå§‹åŒ–å¥åº·ç›‘æ§
    monitor = HealthMonitorService(
        check_interval=1,
        failure_threshold=3,
        recovery_threshold=2,
        timeout=1
    )
    
    # æ¨¡æ‹Ÿ AI æœåŠ¡å¤±è´¥
    async def mock_check_health():
        return False
    
    original_check = monitor.check_ai_health
    monitor.check_ai_health = mock_check_health
    
    try:
        # æ‰§è¡Œ 3 æ¬¡å¥åº·æ£€æŸ¥ï¼ˆè¾¾åˆ°å¤±è´¥é˜ˆå€¼ï¼‰
        for i in range(3):
            await monitor._perform_health_check()
            print(f"  æ£€æŸ¥ {i+1}: å¤±è´¥ï¼Œè¿ç»­å¤±è´¥æ¬¡æ•°: {monitor.consecutive_failures}")
        
        # éªŒè¯å·²åˆ‡æ¢åˆ°é™çº§æ¨¡å¼
        assert monitor.mode == "fallback", "åº”è¯¥åˆ‡æ¢åˆ°é™çº§æ¨¡å¼"
        assert monitor.consecutive_failures == 3, "è¿ç»­å¤±è´¥æ¬¡æ•°åº”ä¸º 3"
        print("  âœ“ æˆåŠŸåˆ‡æ¢åˆ°é™çº§æ¨¡å¼")
        
        # éªŒè¯é™çº§ç»Ÿè®¡
        status = monitor.get_health_status()
        assert status.fallback_statistics["total_fallback_events"] == 1, "åº”è¯¥è®°å½• 1 æ¬¡é™çº§äº‹ä»¶"
        print(f"  âœ“ é™çº§ç»Ÿè®¡æ­£ç¡®: {status.fallback_statistics['total_fallback_events']} æ¬¡é™çº§äº‹ä»¶")
    
    finally:
        monitor.check_ai_health = original_check


# ============================================================================
# æµ‹è¯•2: é™çº§æ¨¡å¼ä½¿ç”¨æœ¬åœ°å¼•æ“
# ============================================================================

async def test_fallback_mode_uses_local_engine():
    """æµ‹è¯•é™çº§æ¨¡å¼ä½¿ç”¨æœ¬åœ°å¼•æ“"""
    print("\n=== æµ‹è¯•2: é™çº§æ¨¡å¼ä½¿ç”¨æœ¬åœ°å¼•æ“ ===")
    
    # åˆå§‹åŒ–æœ¬åœ°è§„åˆ™å¼•æ“
    engine = init_local_rules_engine("backend/config/validation_rules.json")
    await engine.config_manager.load_config()
    
    # æµ‹è¯•æ–‡æ¡£
    test_content = """
    å…³äºä¿¡è®¿äº‹é¡¹çš„ç­”å¤
    
    ä¸»é€å•ä½ï¼šå¸‚ä¿¡è®¿å±€
    
    æ­£æ–‡ï¼š
    ç»è°ƒæŸ¥æ ¸å®ï¼Œæ‚¨åæ˜ çš„é—®é¢˜å±å®ã€‚
    
    è½æ¬¾ï¼š
    XXå•ä½
    2024å¹´1æœˆ2æ—¥
    """
    
    # æ‰§è¡ŒéªŒè¯
    result = await engine.validate_document(test_content)
    
    # éªŒè¯ç»“æœ
    assert result is not None, "åº”è¯¥è¿”å›éªŒè¯ç»“æœ"
    assert result.execution_time < 3.0, f"æ‰§è¡Œæ—¶é—´åº”å°äº 3 ç§’ï¼Œå®é™…: {result.execution_time:.3f}ç§’"
    assert result.rules_executed > 0, "åº”è¯¥æ‰§è¡Œäº†è§„åˆ™"
    
    print(f"  âœ“ æœ¬åœ°å¼•æ“éªŒè¯æˆåŠŸ")
    print(f"    - æ‰§è¡Œæ—¶é—´: {result.execution_time:.3f}ç§’")
    print(f"    - æ‰§è¡Œè§„åˆ™: {result.rules_executed} ä¸ª")
    print(f"    - å‘ç°é—®é¢˜: {len(result.errors)} ä¸ª")
    print(f"    - éªŒè¯ç»“æœ: {'é€šè¿‡' if result.success else 'æœªé€šè¿‡'}")


# ============================================================================
# æµ‹è¯•3: é™çº§é€šçŸ¥åŒ…å«åœ¨å“åº”ä¸­
# ============================================================================

async def test_fallback_notice_in_response():
    """æµ‹è¯•é™çº§é€šçŸ¥åŒ…å«åœ¨å“åº”ä¸­"""
    print("\n=== æµ‹è¯•3: é™çº§é€šçŸ¥åŒ…å«åœ¨å“åº”ä¸­ ===")
    
    # åˆå§‹åŒ–å¥åº·ç›‘æ§
    monitor = init_health_monitor(
        check_interval=1,
        failure_threshold=3,
        recovery_threshold=2,
        timeout=1
    )
    
    # æ¨¡æ‹Ÿé™çº§æ¨¡å¼
    monitor.mode = "fallback"
    monitor.fallback_start_time = time.time()
    
    # éªŒè¯é™çº§çŠ¶æ€
    assert monitor.is_fallback_mode(), "åº”è¯¥å¤„äºé™çº§æ¨¡å¼"
    
    # è·å–æ¢å¤æ—¶é—´ä¼°ç®—
    recovery_time = monitor.get_estimated_recovery_time()
    assert recovery_time is not None, "åº”è¯¥æä¾›æ¢å¤æ—¶é—´ä¼°ç®—"
    
    print(f"  âœ“ é™çº§æ¨¡å¼å·²æ¿€æ´»")
    print(f"    - é¢„è®¡æ¢å¤æ—¶é—´: {recovery_time} ç§’")
    
    # æ¨¡æ‹Ÿå“åº”æ•°æ®
    response_data = {
        "fallback_mode": True,
        "fallback_notice": "AI æœåŠ¡å½“å‰ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°è§„åˆ™åº“è¿›è¡ŒåŸºç¡€æ ¡éªŒ",
        "estimated_recovery": recovery_time
    }
    
    assert response_data["fallback_mode"] is True, "å“åº”åº”æ ‡è®°é™çº§æ¨¡å¼"
    assert response_data["fallback_notice"] is not None, "å“åº”åº”åŒ…å«é™çº§é€šçŸ¥"
    print(f"  âœ“ é™çº§é€šçŸ¥æ­£ç¡®: {response_data['fallback_notice']}")


# ============================================================================
# æµ‹è¯•4: AI æœåŠ¡æ¢å¤è§¦å‘æ­£å¸¸æ¨¡å¼
# ============================================================================

async def test_ai_recovery_triggers_normal_mode():
    """æµ‹è¯• AI æœåŠ¡æ¢å¤è§¦å‘æ­£å¸¸æ¨¡å¼"""
    print("\n=== æµ‹è¯•4: AI æœåŠ¡æ¢å¤è§¦å‘æ­£å¸¸æ¨¡å¼ ===")
    
    # åˆå§‹åŒ–å¥åº·ç›‘æ§
    monitor = HealthMonitorService(
        check_interval=1,
        failure_threshold=3,
        recovery_threshold=2,
        timeout=1
    )
    
    # å…ˆè¿›å…¥é™çº§æ¨¡å¼
    monitor.mode = "fallback"
    monitor.fallback_start_time = time.time()
    monitor.consecutive_failures = 3
    monitor.consecutive_successes = 0
    
    print(f"  åˆå§‹çŠ¶æ€: {monitor.mode} æ¨¡å¼")
    
    # æ¨¡æ‹Ÿ AI æœåŠ¡æ¢å¤
    async def mock_check_health():
        return True
    
    original_check = monitor.check_ai_health
    monitor.check_ai_health = mock_check_health
    
    try:
        # æ‰§è¡Œ 2 æ¬¡å¥åº·æ£€æŸ¥ï¼ˆè¾¾åˆ°æ¢å¤é˜ˆå€¼ï¼‰
        for i in range(2):
            await monitor._perform_health_check()
            print(f"  æ£€æŸ¥ {i+1}: æˆåŠŸï¼Œè¿ç»­æˆåŠŸæ¬¡æ•°: {monitor.consecutive_successes}")
        
        # éªŒè¯å·²åˆ‡æ¢å›æ­£å¸¸æ¨¡å¼
        assert monitor.mode == "normal", "åº”è¯¥åˆ‡æ¢å›æ­£å¸¸æ¨¡å¼"
        assert monitor.consecutive_successes == 2, "è¿ç»­æˆåŠŸæ¬¡æ•°åº”ä¸º 2"
        assert monitor.consecutive_failures == 0, "è¿ç»­å¤±è´¥æ¬¡æ•°åº”é‡ç½®ä¸º 0"
        print("  âœ“ æˆåŠŸåˆ‡æ¢å›æ­£å¸¸æ¨¡å¼")
    
    finally:
        monitor.check_ai_health = original_check


# ============================================================================
# æµ‹è¯•5: é™çº§æŒç»­æ—¶é—´è·Ÿè¸ª
# ============================================================================

async def test_fallback_duration_tracking():
    """æµ‹è¯•é™çº§æŒç»­æ—¶é—´è·Ÿè¸ª"""
    print("\n=== æµ‹è¯•5: é™çº§æŒç»­æ—¶é—´è·Ÿè¸ª ===")
    
    # åˆå§‹åŒ–å¥åº·ç›‘æ§
    monitor = HealthMonitorService(
        check_interval=1,
        failure_threshold=3,
        recovery_threshold=2,
        timeout=1
    )
    
    # è¿›å…¥é™çº§æ¨¡å¼
    monitor.mode = "fallback"
    start_time = time.time()
    monitor.fallback_start_time = start_time
    
    # ç­‰å¾…ä¸€æ®µæ—¶é—´
    await asyncio.sleep(0.5)
    
    # è·å–å½“å‰é™çº§æŒç»­æ—¶é—´
    status = monitor.get_health_status()
    current_duration = status.fallback_statistics.get("current_fallback_duration")
    
    assert current_duration is not None, "åº”è¯¥è·Ÿè¸ªå½“å‰é™çº§æŒç»­æ—¶é—´"
    assert current_duration >= 0, "æŒç»­æ—¶é—´åº”è¯¥æ˜¯éè´Ÿæ•°"
    
    print(f"  âœ“ é™çº§æŒç»­æ—¶é—´è·Ÿè¸ªæ­£ç¡®: {current_duration} ç§’")
    
    # åˆ‡æ¢å›æ­£å¸¸æ¨¡å¼
    monitor.mode = "normal"
    duration = time.time() - start_time
    monitor.total_fallback_duration += duration
    monitor.fallback_start_time = None
    
    # éªŒè¯æ€»é™çº§æ—¶é—´
    status = monitor.get_health_status()
    total_duration = status.fallback_statistics.get("total_fallback_duration")
    
    assert total_duration > 0, "åº”è¯¥è®°å½•æ€»é™çº§æ—¶é—´"
    print(f"  âœ“ æ€»é™çº§æ—¶é—´è®°å½•æ­£ç¡®: {total_duration:.3f} ç§’")


# ============================================================================
# æµ‹è¯•6: é™çº§æ¨¡å¼ä¸‹çš„å¹¶å‘éªŒè¯
# ============================================================================

async def test_concurrent_validations_in_fallback_mode():
    """æµ‹è¯•é™çº§æ¨¡å¼ä¸‹çš„å¹¶å‘éªŒè¯"""
    print("\n=== æµ‹è¯•6: é™çº§æ¨¡å¼ä¸‹çš„å¹¶å‘éªŒè¯ ===")
    
    # åˆå§‹åŒ–æœ¬åœ°è§„åˆ™å¼•æ“
    engine = init_local_rules_engine("backend/config/validation_rules.json")
    await engine.config_manager.load_config()
    
    # æµ‹è¯•æ–‡æ¡£
    test_content = "æµ‹è¯•æ–‡æ¡£å†…å®¹" * 100
    
    # å¹¶å‘æ‰§è¡Œå¤šä¸ªéªŒè¯
    num_concurrent = 5
    start_time = time.time()
    
    tasks = [
        engine.validate_document(test_content)
        for _ in range(num_concurrent)
    ]
    
    results = await asyncio.gather(*tasks)
    
    elapsed_time = time.time() - start_time
    
    # éªŒè¯æ‰€æœ‰ç»“æœ
    assert len(results) == num_concurrent, f"åº”è¯¥è¿”å› {num_concurrent} ä¸ªç»“æœ"
    
    for i, result in enumerate(results):
        assert result is not None, f"ç»“æœ {i+1} ä¸åº”ä¸ºç©º"
        assert result.execution_time < 3.0, f"ç»“æœ {i+1} æ‰§è¡Œæ—¶é—´åº”å°äº 3 ç§’"
    
    avg_time = elapsed_time / num_concurrent
    
    print(f"  âœ“ å¹¶å‘éªŒè¯æˆåŠŸ")
    print(f"    - å¹¶å‘æ•°: {num_concurrent}")
    print(f"    - æ€»è€—æ—¶: {elapsed_time:.3f}ç§’")
    print(f"    - å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
    print(f"    - æ‰€æœ‰éªŒè¯å‡åœ¨ 3 ç§’å†…å®Œæˆ")


# ============================================================================
# æµ‹è¯•7: é…ç½®é‡è½½ï¼ˆæ— éœ€é‡å¯ï¼‰
# ============================================================================

async def test_config_reload_without_restart():
    """æµ‹è¯•æ— éœ€é‡å¯çš„é…ç½®é‡è½½"""
    print("\n=== æµ‹è¯•7: é…ç½®é‡è½½ï¼ˆæ— éœ€é‡å¯ï¼‰===")
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    config_manager = RulesConfigManager("backend/config/validation_rules.json")
    initial_config = await config_manager.load_config()
    
    assert initial_config is not None, "åˆå§‹é…ç½®åº”è¯¥åŠ è½½æˆåŠŸ"
    initial_rule_count = len(initial_config.rules)
    
    print(f"  åˆå§‹é…ç½®: {initial_rule_count} ä¸ªè§„åˆ™")
    
    # é‡æ–°åŠ è½½é…ç½®
    reloaded_config = await config_manager.load_config()
    
    assert reloaded_config is not None, "é‡è½½é…ç½®åº”è¯¥æˆåŠŸ"
    assert len(reloaded_config.rules) == initial_rule_count, "è§„åˆ™æ•°é‡åº”è¯¥ä¸€è‡´"
    
    print(f"  âœ“ é…ç½®é‡è½½æˆåŠŸ: {len(reloaded_config.rules)} ä¸ªè§„åˆ™")


# ============================================================================
# æµ‹è¯•8: åŠ¨æ€å¯ç”¨/ç¦ç”¨è§„åˆ™
# ============================================================================

async def test_rule_toggle_without_restart():
    """æµ‹è¯•åŠ¨æ€å¯ç”¨/ç¦ç”¨è§„åˆ™"""
    print("\n=== æµ‹è¯•8: åŠ¨æ€å¯ç”¨/ç¦ç”¨è§„åˆ™ ===")
    
    # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
    config_manager = RulesConfigManager("backend/config/validation_rules.json")
    await config_manager.load_config()
    
    # è·å–ç¬¬ä¸€ä¸ªè§„åˆ™
    rules = config_manager.get_enabled_rules()
    assert len(rules) > 0, "åº”è¯¥æœ‰å¯ç”¨çš„è§„åˆ™"
    
    first_rule = rules[0]
    rule_id = first_rule.id
    initial_state = first_rule.enabled
    
    print(f"  è§„åˆ™ {rule_id} åˆå§‹çŠ¶æ€: {'å¯ç”¨' if initial_state else 'ç¦ç”¨'}")
    
    # åˆ‡æ¢è§„åˆ™çŠ¶æ€
    success = config_manager.toggle_rule(rule_id, not initial_state)
    assert success, "åˆ‡æ¢è§„åˆ™çŠ¶æ€åº”è¯¥æˆåŠŸ"
    
    # éªŒè¯çŠ¶æ€å·²æ”¹å˜
    updated_rule = next((r for r in config_manager.current_config.rules if r.id == rule_id), None)
    
    assert updated_rule is not None, "åº”è¯¥æ‰¾åˆ°æ›´æ–°åçš„è§„åˆ™"
    assert updated_rule.enabled == (not initial_state), "è§„åˆ™çŠ¶æ€åº”è¯¥å·²æ”¹å˜"
    
    print(f"  âœ“ è§„åˆ™çŠ¶æ€åˆ‡æ¢æˆåŠŸ: {'å¯ç”¨' if updated_rule.enabled else 'ç¦ç”¨'}")
    
    # æ¢å¤åŸå§‹çŠ¶æ€
    config_manager.toggle_rule(rule_id, initial_state)
    print(f"  âœ“ è§„åˆ™çŠ¶æ€å·²æ¢å¤")
    print(f"  âœ“ è§„åˆ™çŠ¶æ€å·²æ¢å¤")


# ============================================================================
# æµ‹è¯•9: æ‰§è¡Œæ—¶é—´è·Ÿè¸ª
# ============================================================================

async def test_execution_time_tracking():
    """æµ‹è¯•æ‰§è¡Œæ—¶é—´è·Ÿè¸ª"""
    print("\n=== æµ‹è¯•9: æ‰§è¡Œæ—¶é—´è·Ÿè¸ª ===")
    
    # åˆå§‹åŒ–æœ¬åœ°è§„åˆ™å¼•æ“
    engine = init_local_rules_engine("backend/config/validation_rules.json")
    await engine.config_manager.load_config()
    
    # æ‰§è¡Œå¤šæ¬¡éªŒè¯
    test_content = "æµ‹è¯•æ–‡æ¡£å†…å®¹" * 100
    num_validations = 5
    
    for i in range(num_validations):
        await engine.validate_document(test_content)
    
    # è·å–æ€§èƒ½æŒ‡æ ‡
    metrics = engine.get_performance_metrics()
    
    assert metrics["total_validations"] == num_validations, f"åº”è¯¥è®°å½• {num_validations} æ¬¡éªŒè¯"
    assert metrics["total_execution_time"] > 0, "åº”è¯¥è®°å½•æ€»æ‰§è¡Œæ—¶é—´"
    assert metrics["average_execution_time"] > 0, "åº”è¯¥è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´"
    
    print(f"  âœ“ æ€§èƒ½æŒ‡æ ‡è·Ÿè¸ªæ­£ç¡®")
    print(f"    - æ€»éªŒè¯æ¬¡æ•°: {metrics['total_validations']}")
    print(f"    - æ€»æ‰§è¡Œæ—¶é—´: {metrics['total_execution_time']:.3f}ç§’")
    print(f"    - å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics['average_execution_time']:.3f}ç§’")


# ============================================================================
# æµ‹è¯•10: æ…¢è§„åˆ™è¯†åˆ«
# ============================================================================

async def test_slow_rule_identification():
    """æµ‹è¯•æ…¢è§„åˆ™è¯†åˆ«"""
    print("\n=== æµ‹è¯•10: æ…¢è§„åˆ™è¯†åˆ« ===")
    
    # åˆå§‹åŒ–æœ¬åœ°è§„åˆ™å¼•æ“
    engine = init_local_rules_engine("backend/config/validation_rules.json")
    await engine.config_manager.load_config()
    
    # æ‰§è¡ŒéªŒè¯
    test_content = "æµ‹è¯•æ–‡æ¡£å†…å®¹" * 100
    await engine.validate_document(test_content)
    
    # è·å–æ…¢è§„åˆ™
    metrics = engine.get_performance_metrics()
    slow_rules = metrics.get("slow_rules", [])
    
    # æ…¢è§„åˆ™é˜ˆå€¼æ˜¯ 500msï¼Œæ­£å¸¸æƒ…å†µä¸‹ä¸åº”è¯¥æœ‰æ…¢è§„åˆ™
    print(f"  âœ“ æ…¢è§„åˆ™æ£€æµ‹å®Œæˆ")
    print(f"    - æ…¢è§„åˆ™æ•°é‡: {len(slow_rules)}")
    
    if slow_rules:
        print(f"    - æ…¢è§„åˆ™è¯¦æƒ…:")
        for rule in slow_rules[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            print(f"      * {rule.get('rule_name')}: {rule.get('average_time', 0):.3f}ç§’")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•"""
    print("=" * 70)
    print("æœ¬åœ°è§„åˆ™åº“é™çº§åŠŸèƒ½ - é›†æˆæµ‹è¯•")
    print("=" * 70)
    
    runner = TestRunner()
    
    # å®šä¹‰æ‰€æœ‰æµ‹è¯•
    tests = [
        ("AI æœåŠ¡å¤±è´¥è§¦å‘é™çº§", test_ai_failure_triggers_fallback),
        ("é™çº§æ¨¡å¼ä½¿ç”¨æœ¬åœ°å¼•æ“", test_fallback_mode_uses_local_engine),
        ("é™çº§é€šçŸ¥åŒ…å«åœ¨å“åº”ä¸­", test_fallback_notice_in_response),
        ("AI æœåŠ¡æ¢å¤è§¦å‘æ­£å¸¸æ¨¡å¼", test_ai_recovery_triggers_normal_mode),
        ("é™çº§æŒç»­æ—¶é—´è·Ÿè¸ª", test_fallback_duration_tracking),
        ("é™çº§æ¨¡å¼ä¸‹çš„å¹¶å‘éªŒè¯", test_concurrent_validations_in_fallback_mode),
        ("é…ç½®é‡è½½ï¼ˆæ— éœ€é‡å¯ï¼‰", test_config_reload_without_restart),
        ("åŠ¨æ€å¯ç”¨/ç¦ç”¨è§„åˆ™", test_rule_toggle_without_restart),
        ("æ‰§è¡Œæ—¶é—´è·Ÿè¸ª", test_execution_time_tracking),
        ("æ…¢è§„åˆ™è¯†åˆ«", test_slow_rule_identification),
    ]
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    for test_name, test_func in tests:
        runner.run_test(test_name, test_func)
    
    # æ‰“å°æ€»ç»“
    return runner.print_summary()


if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)
